{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "def get_non_linearity(activation: str):\n",
    "    if activation == \"LeakyReLU\":\n",
    "        non_linearity = torch.nn.LeakyReLU()\n",
    "    else:\n",
    "        non_linearity = torch.nn.ReLU()\n",
    "    return non_linearity\n",
    "\n",
    "\n",
    "class BaseConvolutionBlock(torch.nn.Module):\n",
    "    def __init__(self, ch_in, ch_out: int, k_size: int, activation=\"LeakyReLU\", dropout: float = 0, bias: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv1d(ch_in, ch_out, k_size, padding=k_size//2, bias=bias)\n",
    "        self.non_linearity = get_non_linearity(activation)\n",
    "        self.dropout = torch.nn.Dropout1d(p=dropout)\n",
    "\n",
    "    def forward(self, x_in: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x_in)  # [N, ch_in, T] -> [N, ch_in+channels_extension, T]\n",
    "        x = self.non_linearity(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderStage(torch.nn.Module):\n",
    "    \"\"\"Conv (and extend channels), downsample 2 by skipping samples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ch_in: int, ch_out: int, k_size: int = 15, dropout: float = 0, bias: bool = True) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block = BaseConvolutionBlock(ch_in, ch_out, k_size=k_size, dropout=dropout, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "\n",
    "        x_ds = x[..., ::2]\n",
    "        # ch_out = ch_in+channels_extension\n",
    "        return x, x_ds\n",
    "\n",
    "\n",
    "class DecoderStage(torch.nn.Module):\n",
    "    \"\"\"Upsample by 2, Concatenate with skip connection, Conv (and shrink channels)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ch_in: int, ch_out: int, k_size: int = 5, dropout: float = 0., bias: bool = True) -> None:\n",
    "        \"\"\"Decoder stage\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.conv_block = BaseConvolutionBlock(ch_in, ch_out, k_size=k_size, dropout=dropout, bias=bias)\n",
    "        self.upsample = torch.nn.Upsample(scale_factor=2, mode=\"linear\", align_corners=True)\n",
    "\n",
    "    def forward(self, x_ds: torch.Tensor, x_skip: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        x_us = self.upsample(x_ds)  # [N, ch, T/2] -> [N, ch, T]\n",
    "        x = torch.cat([x_us, x_skip], dim=1)  # [N, 2.ch, T]\n",
    "        x = self.conv_block(x)  # [N, ch_out, T]\n",
    "        return x\n",
    "\n",
    "\n",
    "class WaveUNet(SeparationModel):\n",
    "    \"\"\"UNET in temporal domain (waveform)\n",
    "    = Multiscale convolutional neural network for audio separation\n",
    "    https://arxiv.org/abs/1806.03185\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 ch_in: int = 1,\n",
    "                 ch_out: int = 2,\n",
    "                 channels_extension: int = 24,\n",
    "                 k_conv_ds: int = 15,\n",
    "                 k_conv_us: int = 5,\n",
    "                 num_layers: int = 6,\n",
    "                 dropout: float = 0.0,\n",
    "                 bias: bool = True,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.need_split = ch_out != ch_in\n",
    "        self.ch_out = ch_out\n",
    "        self.encoder_list = torch.nn.ModuleList()\n",
    "        self.decoder_list = torch.nn.ModuleList()\n",
    "        # Defining first encoder\n",
    "        self.encoder_list.append(EncoderStage(ch_in, channels_extension, k_size=k_conv_ds, dropout=dropout, bias=bias))\n",
    "        for level in range(1, num_layers+1):\n",
    "            ch_i = level*channels_extension\n",
    "            ch_o = (level+1)*channels_extension\n",
    "            if level < num_layers:\n",
    "                # Skipping last encoder since we defined the first one outside the loop\n",
    "                self.encoder_list.append(EncoderStage(ch_i, ch_o, k_size=k_conv_ds, dropout=dropout, bias=bias))\n",
    "            self.decoder_list.append(DecoderStage(ch_o+ch_i, ch_i, k_size=k_conv_us, dropout=dropout, bias=bias))\n",
    "        self.bottleneck = BaseConvolutionBlock(\n",
    "            num_layers*channels_extension,\n",
    "            (num_layers+1)*channels_extension,\n",
    "            k_size=k_conv_ds,\n",
    "            dropout=dropout,\n",
    "            bias=bias)\n",
    "        self.dropout = torch.nn.Dropout1d(p=dropout)\n",
    "        self.target_modality_conv = torch.nn.Conv1d(\n",
    "            channels_extension+ch_in, ch_out, 1, bias=bias)  # conv1x1 channel mixer\n",
    "\n",
    "    def forward(self, x_in: torch.Tensor) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"Forward UNET pass\n",
    "\n",
    "        ```\n",
    "        (1  , 2048)----------------->(24 , 2048) > (1  , 2048)\n",
    "            v                            ^\n",
    "        (24 , 1024)----------------->(48 , 1024)\n",
    "            v                            ^\n",
    "        (48 , 512 )----------------->(72 , 512 )\n",
    "            v                            ^\n",
    "        (72 , 256 )----------------->(96 , 256 )\n",
    "            v                            ^\n",
    "        (96 , 128 )----BOTTLENECK--->(120, 128 )\n",
    "        ```\n",
    "\n",
    "        \"\"\"\n",
    "        skipped_list = []\n",
    "        ds_list = [x_in]\n",
    "        for level, enc in enumerate(self.encoder_list):\n",
    "            x_skip, x_ds = enc(ds_list[-1])\n",
    "            skipped_list.append(x_skip)\n",
    "            ds_list.append(x_ds.clone())\n",
    "            # print(x_skip.shape, x_ds.shape)\n",
    "        x_dec = self.bottleneck(ds_list[-1])\n",
    "        for level, dec in enumerate(self.decoder_list[::-1]):\n",
    "            x_dec = dec(x_dec, skipped_list[-1-level])\n",
    "        # print(x_dec.shape)\n",
    "        x_dec = torch.cat([x_dec, x_in], dim=1)\n",
    "        # print(x_dec.shape)\n",
    "        x_dec = self.dropout(x_dec)\n",
    "        demuxed = self.target_modality_conv(x_dec)\n",
    "        # print(demuxed.shape)\n",
    "        if self.need_split:\n",
    "            return torch.chunk(demuxed, self.ch_out, dim=1)\n",
    "        return demuxed, None\n",
    "\n",
    "        # x_skip, x_ds\n",
    "        # (24, 2048), (24, 1024)\n",
    "        # (48, 1024), (48, 512 )\n",
    "        # (72, 512 ), (72, 256 )\n",
    "        # (96, 256 ), (96, 128 )\n",
    "\n",
    "        # (120, 128 )\n",
    "        # (96 , 256 )\n",
    "        # (72 , 512 )\n",
    "        # (48 , 1024)\n",
    "        # (24 , 2048)\n",
    "        # (25 , 2048) demuxed - after concat\n",
    "        # (1  , 2048)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class SeparationModel(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def count_parameters(self) -> int:\n",
    "        \"\"\"Count the total number of parameters of the model\n",
    "\n",
    "        Returns:\n",
    "            int: total amount of parameters\n",
    "        \"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def receptive_field(self) -> int:\n",
    "        \"\"\"Compute the receptive field of the model\n",
    "\n",
    "        Returns:\n",
    "            int: receptive field\n",
    "        \"\"\"\n",
    "        input_tensor = torch.rand(1, 1, 4096, requires_grad=True)\n",
    "        out, out_noise = self.forward(input_tensor)\n",
    "        grad = torch.zeros_like(out)\n",
    "        grad[..., out.shape[-1]//2] = torch.nan  # set NaN gradient at the middle of the output\n",
    "        out.backward(gradient=grad)\n",
    "        self.zero_grad()  # reset to avoid future problems\n",
    "        return int(torch.sum(input_tensor.grad.isnan()).cpu())  # Count NaN in the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.audio import SignalDistortionRatio\n",
    "\n",
    "# Exemple de signal cible et estimé\n",
    "target = torch.randn(1, 16000)  # Signal cible (1 second à 16kHz)\n",
    "predicted = torch.randn(1, 16000)  # Signal estimé\n",
    "\n",
    "# Calcul du SDR\n",
    "sdr = SignalDistortionRatio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SignalDistortionRatio.update() got an unexpected keyword argument 'reduction'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m16000\u001b[39m)  \u001b[38;5;66;03m# 10 segments\u001b[39;00m\n\u001b[0;32m      2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m16000\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m sdr_values \u001b[38;5;241m=\u001b[39m \u001b[43msdr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pas de réduction (liste des SDR)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m median_sdr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmedian(sdr_values)\n",
      "File \u001b[1;32mc:\\Users\\antoi\\anaconda3\\envs\\apprentissage_profond_traitement_du_signal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\antoi\\anaconda3\\envs\\apprentissage_profond_traitement_du_signal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\antoi\\anaconda3\\envs\\apprentissage_profond_traitement_du_signal\\Lib\\site-packages\\torchmetrics\\metric.py:316\u001b[0m, in \u001b[0;36mMetric.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_full_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_reduce_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache\n",
      "File \u001b[1;32mc:\\Users\\antoi\\anaconda3\\envs\\apprentissage_profond_traitement_du_signal\\Lib\\site-packages\\torchmetrics\\metric.py:385\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# allow grads for batch computation\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;66;03m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[1;32m--> 385\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m batch_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# reduce batch and global state\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\antoi\\anaconda3\\envs\\apprentissage_profond_traitement_du_signal\\Lib\\site-packages\\torchmetrics\\metric.py:550\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 550\u001b[0m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    552\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "\u001b[1;31mTypeError\u001b[0m: SignalDistortionRatio.update() got an unexpected keyword argument 'reduction'"
     ]
    }
   ],
   "source": [
    "targets = torch.randn(10, 16000)  # 10 segments\n",
    "predictions = torch.randn(10, 16000)\n",
    "\n",
    "sdr_values = sdr(predictions, targets, reduction='none')  # Pas de réduction (liste des SDR)\n",
    "median_sdr = torch.median(sdr_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 314 but got size 313 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m WaveUNet(ch_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m)\n\u001b[0;32m      2\u001b[0m inp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m80000\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# print(model)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# print(model.count_parameters())\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(out[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\antoi\\anaconda3\\envs\\apprentissage_profond_traitement_du_signal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\antoi\\anaconda3\\envs\\apprentissage_profond_traitement_du_signal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 130\u001b[0m, in \u001b[0;36mWaveUNet.forward\u001b[1;34m(self, x_in)\u001b[0m\n\u001b[0;32m    128\u001b[0m x_dec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbottleneck(ds_list[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m level, dec \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_list[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m--> 130\u001b[0m     x_dec \u001b[38;5;241m=\u001b[39m \u001b[43mdec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_dec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipped_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# print(x_dec.shape)\u001b[39;00m\n\u001b[0;32m    132\u001b[0m x_dec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x_dec, x_in], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\antoi\\anaconda3\\envs\\apprentissage_profond_traitement_du_signal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\antoi\\anaconda3\\envs\\apprentissage_profond_traitement_du_signal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 60\u001b[0m, in \u001b[0;36mDecoderStage.forward\u001b[1;34m(self, x_ds, x_skip)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m     59\u001b[0m x_us \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample(x_ds)  \u001b[38;5;66;03m# [N, ch, T/2] -> [N, ch, T]\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_us\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_skip\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [N, 2.ch, T]\u001b[39;00m\n\u001b[0;32m     61\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_block(x)  \u001b[38;5;66;03m# [N, ch_out, T]\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 314 but got size 313 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = WaveUNet(ch_out=2, num_layers=9)\n",
    "inp = torch.rand(1, 1, 80000)\n",
    "out = model(inp)\n",
    "# print(model)\n",
    "# print(model.count_parameters())\n",
    "print(out[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apprentissage_profond_traitement_du_signal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
